{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Level Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows the core functionalities of this project by creating and training different models. Model created can be trained and saved as pre-trained model for future usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading Required Libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2, os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import pickle\n",
    "import csv\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Detection Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 7 #Total number of Expressions\n",
    "epochs = 15\n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances:  35888\n",
      "Instance length:  2304\n"
     ]
    }
   ],
   "source": [
    "with open(\"fer2013.csv\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "lines = np.array(content)\n",
    "\n",
    "instances = lines.size\n",
    "\n",
    "print(\"Instances: \",instances)\n",
    "print(\"Instance length: \",len(lines[1].split(\",\")[1].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = [], [], [], [] #Creating null arrays for testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,instances):\n",
    "    try:\n",
    "        emotion, pixel, type_class = lines[i].split(\",\")\n",
    "          \n",
    "        val = pixel.split(\" \")\n",
    "            \n",
    "        pixels_float = np.array(val, 'float32')\n",
    "        \n",
    "        emotion = keras.utils.to_categorical(emotion, classes)\n",
    "    \n",
    "        if 'Training' in type_class:\n",
    "            \n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels_float)\n",
    "            \n",
    "        elif 'PublicTest' in type_class:\n",
    "            \n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels_float)\n",
    "            \n",
    "    except:\n",
    "        print(\"\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709 train samples\n",
      "3589 test samples\n"
     ]
    }
   ],
   "source": [
    "#Putting values in numpy arrays\n",
    "\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "#Normalizing the values\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#1st convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch process\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "128/128 [==============================] - 16s 128ms/step - loss: 1.8230 - acc: 0.2445\n",
      "Epoch 2/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.8010 - acc: 0.2518\n",
      "Epoch 3/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.7380 - acc: 0.2808\n",
      "Epoch 4/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.6549 - acc: 0.3374\n",
      "Epoch 5/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.5974 - acc: 0.3646\n",
      "Epoch 6/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.5373 - acc: 0.3957\n",
      "Epoch 7/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.5204 - acc: 0.4060\n",
      "Epoch 8/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.4465 - acc: 0.4380\n",
      "Epoch 9/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.4246 - acc: 0.4449\n",
      "Epoch 10/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.3786 - acc: 0.4655\n",
      "Epoch 11/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.3451 - acc: 0.4803\n",
      "Epoch 12/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.3211 - acc: 0.4949\n",
      "Epoch 13/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.2916 - acc: 0.5034\n",
      "Epoch 14/15\n",
      "128/128 [==============================] - 15s 118ms/step - loss: 1.2585 - acc: 0.5173\n",
      "Epoch 15/15\n",
      "128/128 [==============================] - 15s 117ms/step - loss: 1.2486 - acc: 0.5267\n"
     ]
    }
   ],
   "source": [
    "fit = True\n",
    "\n",
    "if fit == True:\n",
    "    #model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\n",
    "    model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one\n",
    "else:\n",
    "    model.load_weights('facial_expression_model_weights.h5') #load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 384/3589 [==>...........................] - ETA: 2s"
     ]
    }
   ],
   "source": [
    "#overall evaluation\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Following are the names of subjects on which the face recognition model has been trained on. More names can be added\n",
    "here as model learns to predict on more faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of subjects for face recognition part\n",
    "subjects = [\"\", \"Anubhav Kumar\",\"Hemant Toshniwal\",\"Arvind Kumar\",\"Gitesh Bhasin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to detect face using OpenCV\n",
    "def detect_face(img):\n",
    "    #converting the test image to gray image as opencv face detector expects gray images\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #loading OpenCV face detector, I am using LBP which is fast\n",
    "    face_cascade = cv2.CascadeClassifier('lbpcascade_frontalface.xml')\n",
    "\n",
    "    #Detecting multiscale (some images may be closer to camera than others) images\n",
    "    #result is a list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);\n",
    "    \n",
    "    #if no faces are detected then return original img\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    "    \n",
    "    #under the assumption that there will be only one face,\n",
    "    #extracting the face area\n",
    "    (x, y, w, h) = faces[0]\n",
    "    \n",
    "    #returning only the face part of the image\n",
    "    return gray[y:y+w, x:x+h], faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will read all persons' training images, detect face from each image\n",
    "#and will return two lists of exactly same size, one list \n",
    "# of faces and another list of labels for each face\n",
    "def prepare_training_data(data_folder_path):\n",
    "    \n",
    "    #------STEP-1--------\n",
    "    #getting the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "    \n",
    "    #list to hold all subject faces\n",
    "    faces = []\n",
    "    #list to hold labels for all subjects\n",
    "    labels = []\n",
    "    \n",
    "    #Going through each directory and reading images within it\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        #our subject directories start with letter 's' so\n",
    "        #ignoring any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue;\n",
    "            \n",
    "        #------STEP-2--------\n",
    "        #extracting label number of subject from dir_name\n",
    "        #format of dir name = slabel\n",
    "        #so removing letter 's' from dir_name will give us label\n",
    "        label = int(dir_name.replace(\"s\", \"\"))\n",
    "        \n",
    "        #building path of directory containing images for current subject\n",
    "        subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    "        \n",
    "        #getting the images names that are inside the given subject directory\n",
    "        subject_images_names = os.listdir(subject_dir_path)\n",
    "        \n",
    "        #------STEP-3--------\n",
    "        #going through each image name, read image, \n",
    "        #detecting and adding face to list of faces\n",
    "        for image_name in subject_images_names:\n",
    "            \n",
    "            #ignore system files like .DS_Store\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue;\n",
    "            \n",
    "            #building image path\n",
    "            image_path = subject_dir_path + \"/\" + image_name\n",
    "\n",
    "            #reading image\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            #displaying an image window to show the image \n",
    "            cv2.imshow(\"Training on image...\", image)\n",
    "            cv2.waitKey(100)\n",
    "            \n",
    "            #detecting face\n",
    "            face, rect = detect_face(image)\n",
    "            \n",
    "            #------STEP-4--------\n",
    "            if face is not None:\n",
    "                #adding face to list of faces\n",
    "                faces.append(face)\n",
    "                #adding label for this face\n",
    "                labels.append(label)\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return faces, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    "\n",
    "#printing total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using **Local binary pattern** histogram from opencv module. It is the one out of three libraries in opencv\n",
    "package that helps in face recognition. LBPH is a better pick over others because it is not affected by the illumination \n",
    "of images and tries to find local features in images rather than looking at all the images at once. Right now the only limitation of using this model is that we are unable to set a minimum threshold value. Due to this every subject is identified as one of the trained subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our LBPH face recognizer \n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training our face recognizer of our training faces\n",
    "face_recognizer.train(faces, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw rectangle on image \n",
    "def draw_rectangle(img, rect):\n",
    "    (x, y, w, h) = rect\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "#function to draw text on give image\n",
    "def draw_text(img, text, x, y):\n",
    "    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function recognizes the person in image passed\n",
    "#and draws a rectangle around detected face with name of the \n",
    "#subject\n",
    "def predict(test_img):\n",
    "    #make a copy of the image as we don't want to chang original image\n",
    "    img = test_img.copy()\n",
    "    #detect face from the image\n",
    "    face, rect = detect_face(img)\n",
    "\n",
    "    #predict the image using our face recognizer \n",
    "    label= face_recognizer.predict(face)\n",
    "    #get name of respective label returned by face recognizer\n",
    "    label_text = subjects[label]\n",
    "    #label_text = label\n",
    "    #draw a rectangle around face detected\n",
    "    draw_rectangle(img, rect)\n",
    "    #draw name of predicted person\n",
    "    draw_text(img, label_text, rect[0], rect[1]-5)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Uses of each function**\n",
    "\n",
    "- **eye_aspect_ratio** : This function helps in calculating the euclidean distance between the vertical and horizontal landmarks of of eye hence giving us eye aspect ratio. \n",
    "\n",
    "\n",
    "- **aperture** : This function helps in calculating the area to a given eye (Left eye in our case).It does so using the co-ordinates of landmarks of left eye\n",
    "\n",
    "\n",
    "- **write_csv** : This function writes name of the subject, emotion, percentage of each emotion, blink rate of subject and area of the eye.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Definitions\n",
    "\n",
    "#Calculates distance between vertical and horizontal eye landmarks\n",
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark coordinates\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    # compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "\n",
    "    # return ear\n",
    "    return ear\n",
    "\n",
    "#Calculates eye aperture (Area of eye)\n",
    "def aperture(xx,yy):\n",
    "    f=[]\n",
    "    g=0\n",
    "    a=0\n",
    "    for i in range(0,6):\n",
    "        if i<5:\n",
    "            g += xx[i]*yy[i+1]\n",
    "            a += yy[i]*xx[i+1]\n",
    "        else:\n",
    "            g +=xx[i]*yy[i-5]\n",
    "            a +=xx[i-5]*yy[i]\n",
    "    d=abs(g-a)/2\n",
    "    f.append(d)\n",
    "    \n",
    "    return f\n",
    "\n",
    "#For writing data to CSV\n",
    "def write_csv(data):\n",
    "    #if file exists\n",
    "    if os.path.exists('result.csv'):\n",
    "        with open('result.csv', 'a') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(data)\n",
    "    #if file does not exists\n",
    "    else:\n",
    "        with open('result.csv', 'a') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow([\"name\", \"emotion\", \"percentage\",\"blink\",\"area\"])\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables Definition \n",
    "\n",
    "# Minimum EAR Threshold \n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "# Initializing the frame counters and the total number of blinks\n",
    "COUNTER = 0\n",
    "TOTAL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using front face detector function from dlib module to detect front faces and loading pre-trained 68 face landmark \n",
    "predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading facial landmark predictor \n",
    "\n",
    "# initializing dlib's face detector (HOG-based) and then create the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor2 = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding out the 6 co-ordinates of each eye in order to calculate area as well as blink rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the indexes for the left and right eye, respectively\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use haarcascade frontalface XML provided by opencv that helps in detecting the faces in the frames. VideoCapture \n",
    "function of opencv can take the input in real time using webcam or it can take the feed from a saved video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading haarcascade XML to detect front face\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#Taking feed from video\n",
    "cap = cv2.VideoCapture('happy.mp4')\n",
    "#Taking feed from camera\n",
    "#cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here all the models are implemented in real time. First the **emotion recognition model** takes the feed from video/camera and predicts the emotion of the subjects in frame. The detectMultiScale function from face cascade helps in detecting multiple faces in one frame. The frame is then resized into **48x48** pixels and then converted into the numpy array which is fed to the model in order to predict the emotion.\n",
    "\n",
    "After this the **face recognition model** runs in order to predict the face found in the frame. The next model **plots 68 landmark** on the face of subjects and then the eye blink rate and eye area is calculated which is later used as a factor to calculate stress level.\n",
    "\n",
    "After all the model finishes their first run all the **outputs are saved to a CSV file**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real-Time Open-CV integration of all models\n",
    "\n",
    "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "summedaperture = []\n",
    "t1=time.time()\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    ##########################################\n",
    "    frame2 = imutils.resize(frame, width=800,height=800)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray2, 0)\n",
    "    ##########################################\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2) #drawing rectangle to main image\n",
    "\n",
    "        detected_face = frame[int(y):int(y+h), int(x):int(x+w)] #cropping detected face\n",
    "        detected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY) #transforming to gray scale\n",
    "        detected_face = cv2.resize(detected_face, (48, 48)) #resizing to 48x48\n",
    "\n",
    "        img_pixels = image.img_to_array(detected_face)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "\n",
    "        img_pixels /= 255 #pixels are in scale of [0, 255]. normalizing all pixels in scale of [0, 1]\n",
    "\n",
    "        predictions = model.predict(img_pixels) #storing probabilities of 7 expressions\n",
    "        \n",
    "        #Face recognition\n",
    "        fpredictions = face_recognizer.predict(detected_face)\n",
    "        label=fpredictions[0]\n",
    "        label_text = subjects[label]\n",
    "        \n",
    "        \n",
    "\n",
    "        #finding max indexed array 0: angry, 1:disgust, 2:fear, 3:happy, 4:sad, 5:surprise, 6:neutral\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotion = emotions[max_index]\n",
    "        #writing emotion text above rectangle\n",
    "        cv2.putText(frame, emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "        cv2.putText(frame, label_text, (int(x), int(y)+150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        write_csv([label_text,emotion,predictions])\n",
    "        \n",
    "        \n",
    "        cv2.imshow('Facial Recognizer', frame)\n",
    "        \n",
    "        for rect in rects:\n",
    "        # determining the facial landmarks for the face region, then\n",
    "        # converting the facial landmark coordinates to a NumPy\n",
    "        # array\n",
    "            shape = predictor2(gray2, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            left_eye=shape[42:48,]\n",
    "\n",
    "            x=[]\n",
    "            y=[]\n",
    "            for k in range(0,6):\n",
    "                a=left_eye[k][0]\n",
    "                x.append(a)\n",
    "                b=left_eye[k][1]\n",
    "                y.append(b)\n",
    "            summedaperture.extend(aperture(x,y))\n",
    "\n",
    "            # extracting the left and right eye coordinates, then using the\n",
    "            # coordinates to compute the eye aspect ratio for both eyes\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "            # averaging the eye aspect ratio together for both eyes\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # computing the convex hull for the left and right eye, then\n",
    "            # visualizing each of the eyes\n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(frame2, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "            cv2.drawContours(frame2, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "            # checking to see if the eye aspect ratio is below the blink\n",
    "            # threshold, and if so, incrementing the blink frame counter\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "\n",
    "            # otherwise, the eye aspect ratio is not below the blink\n",
    "            # threshold\n",
    "            else:\n",
    "                # if the eyes were closed for a sufficient number of\n",
    "                # then incrementing the total number of blinks\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    TOTAL += 1\n",
    "\n",
    "                # resetting the eye frame counter\n",
    "                COUNTER = 0\n",
    "\n",
    "            # drawing the total number of blinks on the frame along with\n",
    "            # the computed eye aspect ratio for the frame\n",
    "            cv2.putText(frame2, \"Blinks: {}\".format(TOTAL), (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            cv2.putText(frame2, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "    # showing the frame\n",
    "    cv2.imshow(\"Eye Counter\", frame2)\n",
    "    if len(summedaperture)!=0:\n",
    "        average_aperture=sum(summedaperture)/len(summedaperture)\n",
    "    \n",
    "    t2=time.time()\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or (t2-t1)>70: #press q to quiit\n",
    "         \n",
    "        write_csv([\" \",\" \",\"\",TOTAL,average_aperture])\n",
    "        break\n",
    "        \n",
    "        \n",
    "#killing open cv things\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below piece of code reads the csv file which we saved earlier and then extracts the most dominating emotion, blink\n",
    "rate and eye area. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading CSV File\n",
    "file=pd.read_csv('result.csv',header=0)\n",
    "\n",
    "#Extracting most common feature from CSV file\n",
    "emotion= file['emotion']\n",
    "emotion_count = emotion.value_counts()\n",
    "max_emotion= emotion_count.index[0]\n",
    "\n",
    "#Extracting Blink Rate from CSV file\n",
    "blink_rate = file['blink']\n",
    "br=blink_rate.dropna()\n",
    "br=br.values[0]\n",
    "\n",
    "#Extracting Eye Area from CSV file\n",
    "eye_area = file['area']\n",
    "ear_area= eye_area.dropna()\n",
    "area=ear_area.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally we classify the stress level based on the output of above models. We classify all the emotions into three \n",
    "categories. \n",
    "\n",
    "- **Not Stressed : If the emotion recognition model predicts that the most dominating emotion is happy then, we straight away classify our subject as not stressed.**\n",
    "\n",
    "- **Mild Stress : If the emotion recognition model predicts that the most dominating emotion is neutral or anger, sad, fear then, we look at below factors:**\n",
    "\n",
    "1) If the identified emotion is neutral and either the eye area lies between 350 to 530 or blink rate per minute lies between 25 to 46 then we say that the subject is under mild stress.\n",
    "2) If the identified emotion is anger,sad, or fear and the eye area does not lies between 350 to 530 and blink rate per minute does not lies between 25 to 46 then we say that the subject is under mild stress.\n",
    "\n",
    "- **Moderate Stress : If the emotion recognition model predicts that the most dominating emotion is neutral then, we look at below factors:**\n",
    "\n",
    "1) If the eye area lies between 350 to 530 and blink rate per minute lies between 25 to 46 then, we say that the subject is under high stress.\n",
    "2) If either of above condition is true then we say that the subject is moderately stressed.\n",
    "\n",
    "- **High Stress : If the emotion recognition model predicts that the most dominating emotion is anger, sad, fear then, we look at below factors:**\n",
    "\n",
    "1) If the eye area lies between 350 to 530 and blink rate per minute lies between 25 to 46 then, we say that the subject is under high stress.\n",
    "2) If either of above condition is true then we say that the subject is moderately stressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification of stress based on various factors and two predictor models\n",
    "def StressClass(max_emotion,br,ear):\n",
    "    happy = 0\n",
    "    neutral = 0\n",
    "    other = 0\n",
    "\n",
    "    #Ranging values\n",
    "    if max_emotion == \"happy\":\n",
    "        happy = 1\n",
    "    elif max_emotion == \"neutral\":\n",
    "        neutral = 1 \n",
    "    else:\n",
    "        other = 1\n",
    "\n",
    "    # If blink rate is between 25 to 46\n",
    "    if br > 25 and br < 46:\n",
    "        br=1\n",
    "    else:\n",
    "        br=0\n",
    "\n",
    "    # If eye aperture is between 350 to 530\n",
    "    if area>350 and area<530:\n",
    "        ear=1\n",
    "    else:\n",
    "        ear=0\n",
    "\n",
    "    #Stress Prediction\n",
    "    if happy == 1:\n",
    "        stress = \"Not Stressed (Happy)\"\n",
    "    elif neutral == 1:\n",
    "        if ear == 0 and br == 0:\n",
    "            stress =\"Not Stressed (Happy)\"\n",
    "        elif ear == 1 and br == 1:\n",
    "            stress = \"Moderate stress\"\n",
    "        else :\n",
    "            stress = \"Mild stress\"\n",
    "    else :\n",
    "        if ear == 0 and br == 0:\n",
    "            stress =\"Mild stress\"\n",
    "        elif ear == 1 and br == 1:\n",
    "            stress = \"High stress\"\n",
    "        else :\n",
    "            stress = \"Moderate stress\"\n",
    "    return stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StressClass(max_emotion,br,ear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
